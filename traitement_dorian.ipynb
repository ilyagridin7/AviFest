{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "import os\n",
    "from datetime import date, timedelta\n",
    "from itertools import tee\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "import pymongo\n",
    "from pymongo.collection import Collection\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, explode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initialisation de MongoDB & Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "client = pymongo.MongoClient(\"mongodb://127.0.0.1:27017/\")\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client.drop_database('AviFest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création de la base de donnée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = client['AviFest']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = db['category']\n",
    "categories.insert_many([\n",
    "    {'type': 'performance', 'price': 10},\n",
    "    {'type': 'indiscipline', 'price': 20},\n",
    "    {'type': 'spectacle', 'price': 30},\n",
    "    {'type': 'théâtre', 'price': 40},\n",
    "    {'type': 'dance', 'price': 50},\n",
    "    {'type': 'installation photographique', 'price': 60},\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for document in categories.find():\n",
    "#     print(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = db['user']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(row):\n",
    "    row['gender'] = row['gender'][0]\n",
    "    firstname, lastname = row['name'].split(' ')\n",
    "    row['firstname'] = firstname\n",
    "    row['lastname'] = lastname\n",
    "    \n",
    "    del row['name']\n",
    "    \n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persons = spark.read.json('json/data.json').withColumn(\"persons\", explode(col(\"persons\"))).select(\n",
    "    \"persons.name\",\n",
    "    \"persons.adress\",\n",
    "    \"persons.phone\",\n",
    "    \"persons.email\",\n",
    "    \"persons.age\",\n",
    "    \"persons.gender\",\n",
    ")\n",
    "\n",
    "df = persons.toPandas().apply(preprocessing, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    users.insert_one({\n",
    "        'phone': row['phone'],\n",
    "        'email': row['email'],\n",
    "        'sexe': row['gender'],\n",
    "        'age': row['age'],\n",
    "        'firstname': row['firstname'],\n",
    "        'lastname': row['lastname'],\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for document in users.find():\n",
    "#     print(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shows = db['show']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.options(header='True', inferSchema='True', delimiter=',') \\\n",
    "  .csv(\"csv/show.csv\") \\\n",
    "  .drop('id_location', 'id_show', 'artist')\n",
    "df = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = spark.read.json('json/locations.json', multiLine=True) \\\n",
    "    .withColumn(\"features\", explode(col(\"features.geometry.coordinates\"))) \\\n",
    "    .withColumn(\"lat\", col('features').getItem(0).alias('lat')) \\\n",
    "    .withColumn(\"long\", col('features').getItem(1).alias('long')) \\\n",
    "    .drop(\"type\", \"features\") \\\n",
    "    .toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current = date(2022, 6, 1)\n",
    "end = date(2022, 7, 1)\n",
    "date_list = []\n",
    "\n",
    "while current < end:\n",
    "    for item in pd.date_range(current+pd.DateOffset(hours=10), current+pd.DateOffset(hours=24), freq='2H'):\n",
    "        date_list.append(str(item))\n",
    "    current += timedelta(days=1)\n",
    "\n",
    "random.shuffle(date_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artists = spark.read.json('json/data.json').withColumn(\"artists\", explode(col(\"artists\"))).select(\n",
    "    \"artists.firstname\",\n",
    "    \"artists.lastname\",\n",
    ")\n",
    "\n",
    "artists = artists.toPandas().sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    lat, long = locations.loc[index % locations.shape[0]].values\n",
    "    shows.insert_one({\n",
    "        'title': row['nom'],\n",
    "        'type': row['type'],\n",
    "        'artist': ' '.join(artists.loc[index % artists.shape[0]].values),\n",
    "        'nbPlace': row['nb_place'],\n",
    "        'lat': lat,\n",
    "        'long': long,\n",
    "        'date' : date_list[index],\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for document in shows.find():\n",
    "#    print(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parkings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parkings = db['parking']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation de 2 générateurs identiques\n",
    "rows, get_nb_rows = tee(spark.read.option(\"multiline\", \"true\").json('json/parkings.json').toPandas().iterrows())\n",
    "nb_parkings = len(list((get_nb_rows)))\n",
    "\n",
    "# Liste de booleans aléatoires avec 25% de False\n",
    "list_pmr = list(map(lambda x: x < 0.75, [random.random() for _ in range(nb_parkings)]))\n",
    "\n",
    "description: str\n",
    "# Boucle sur 3 list différentes, les parkings et 2 listes aléatoires pour générer des booleans\n",
    "# Avec Spark, je peux directement unpack le json, c'est bizarre mais ca marche\n",
    "for (_, (((long, lat), _), (description, name), _)), pmr, bus in zip(rows, list_pmr, reversed(list_pmr)):\n",
    "    \n",
    "    try:\n",
    "        slot = int(re.search(r'\\*\\*(\\d+)\\*\\*', description).group(1))\n",
    "        \n",
    "        if not slot:\n",
    "            slot = 'unknown'\n",
    "    except: slot = 'unknown'\n",
    "    \n",
    "    parkings.insert_one({\n",
    "        'type': 'voiture',\n",
    "        'name': name,\n",
    "        'nbslots': slot,\n",
    "        'paying': not 'gratuit' in description.lower(),\n",
    "        'busFestiv': bus,\n",
    "        'pmr': pmr,\n",
    "        'lat': lat,\n",
    "        'long': long,\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Réservation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reservations = db['reservation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_id_from_collection(collection: Collection):\n",
    "    return str(collection.aggregate([\n",
    "                { '$sample': { 'size': 1 } },\n",
    "                { \"$project\": {\n",
    "                    \"_id\": 1,\n",
    "                }}\n",
    "            ]).next()['_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(20):\n",
    "    reservations.insert_one({\n",
    "        \"id_person\": get_random_id_from_collection(users),\n",
    "        \"id_show\": get_random_id_from_collection(shows),\n",
    "        'created_at': date(2022, 6, 1) + timedelta(days=random.randint(0, 29)),\n",
    "        \"nbreservation\": random.randint(1, 5),\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for document in reservations.find():\n",
    "#    print(document)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "c12bee36989f28e5e1c12fab4f4acca5639cc01196467d9f9512280340dae30a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
