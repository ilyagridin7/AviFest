{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "import json\n",
    "from itertools import tee\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import pymongo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initialisation de MongoDB & Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = pymongo.MongoClient(\"mongodb://127.0.0.1:27017/\")\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "client.drop_database('AviFest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Création de la base de donnée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = client['AviFest']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lecture du fichier persons.json qui contient une liste de personne générée aléatoirement dans le script gen_persons.py.  \n",
    "Traitement et nettoyage des données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json('persons.json').toPandas()\n",
    "\n",
    "def preprocessing(row):\n",
    "    row['sexe'] = row['sexe'][0]\n",
    "    firstname, lastname, *_ = row['name'].split(' ')\n",
    "    row['firstname'] = firstname\n",
    "    row['lastname'] = lastname\n",
    "    \n",
    "    del row['name']\n",
    "    \n",
    "    return row\n",
    "\n",
    "df = df.apply(preprocessing, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insertion dans la collection \"users\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = db['users']\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    users.insert_one({\n",
    "        'phone': row['phone'],\n",
    "        'email': row['email'],\n",
    "        'sexe': row['sexe'],\n",
    "        'age': row['age'],\n",
    "        'firstname': row['firstname'],\n",
    "        'lastname': row['lastname'],\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for document in users.find({}):\n",
    "    print(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lecture du fichier show.csv  \n",
    "Traitement et nettoyage de donnée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.options(header='True', inferSchema='True', delimiter=',') \\\n",
    "  .csv(\"show.csv\")\n",
    "df = df.drop(*filter(lambda x: '_c' in x, df.columns)).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insertion dans la collection \"shows\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shows = db['shows']\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    shows.insert_one({\n",
    "        'title': row['title'],\n",
    "        'type': row['type'],\n",
    "        'artist': row['artist'],\n",
    "        'price': '',\n",
    "        'nbPlace': row['nb_place'],\n",
    "        'idLocation': '',\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for document in shows.find({}):\n",
    "    print(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lecture du fichier parkings.json  \n",
    "Traitement et nettoyage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parkings = db['parkings']\n",
    "\n",
    "rows, get_nb_rows = tee(spark.read.option(\"multiline\", \"true\").json('parkings.json').toPandas().iterrows())\n",
    "nb_parkings = len(list((get_nb_rows)))\n",
    "\n",
    "# Liste de booleans aléatoires avec 25% de False\n",
    "list_pmr = list(map(lambda x: x < 0.75, [random.random() for _ in range(nb_parkings)]))\n",
    "\n",
    "description: str\n",
    "# Boucle sur 3 list différentes, les parkings et 2 listes aléatoires pour générer des booleans\n",
    "# Avec Spark, je peux directement unpack le json, c'est bizarre mais ca marche\n",
    "for (_, (((long, lat), _), (description, name), _)), pmr, bus in zip(rows, list_pmr, reversed(list_pmr)):\n",
    "    \n",
    "    try:\n",
    "        slot = int(re.search(r'\\*\\*(\\d+)\\*\\*', description).group(1))\n",
    "        \n",
    "        if not slot:\n",
    "            slot = 'unknown'\n",
    "    except: slot = 'unknown'\n",
    "    \n",
    "    parkings.insert_one({\n",
    "        'type': 'voiture',\n",
    "        'name': name,\n",
    "        'nbslots': slot,\n",
    "        'paying': not 'gratuit' in description.lower(),\n",
    "        'busFestiv': bus,\n",
    "        'pmr': pmr,\n",
    "        'lat': lat,\n",
    "        'long': long,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for document in parkings.find({}):\n",
    "    print(document)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0rc2 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0rc2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "35149e882e522fd65d9c429f4682805aab11d4e15be92a9167e161bfa5dcde43"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
